source,pq_id,title,abstract,year,author,type,level
proquest_dt,1807632181,A study of trust development in virtual human-agent teamwork without explicit coordination,"With the advancement in agent technology and social recognition and acceptance of autonomous online services we foresee increasing use of agents in social contexts and, in particular, in human-agent virtual and ad hoc team applications. For such virtual ad hoc teams, particularly those where explicit coordination to determine task allocations are infeasible due to communication and time constraints,  to be effective, agents need to quickly develop an understanding of the expectation of human team members and be able to quickly adapt accordingly. This dissertation  empirically investigates the effects of agent performance, past experience of humans, and agent reputation on human trust in and behavior towards agent teammates. We also study the differences in growth of humans' trust attitudes towards human and agent teammates during initial interactions while achieving team goals.    Ad hoc collaboration of humans and agents without pre-planning is a novel human-agent teamwork concept that enables diverse, high-impact applications including emergency response, group buying, online social movements, and next generation crowd-sourcing. In spite of prior work on the influence of various factors on human trust in automation, very little is known about human trust in ""peer"" agent teammates. This is the first study to investigate these factors in virtual human-agent teamwork without pre-planning or explicit coordination.    This dissertation introduces a repeated team coordination game, the Game of Trust (GoT), in which two players repeatedly cooperate to complete team tasks without prior assignment of subtasks. We systematically evaluate the effects of agent performance, past experience, and agent reputation on human trust in the GoT by performing an extensive set of controlled experiments with subjects recruited from Amazon Mechanical Turk, a crowdsourcing Internet marketplace. We collect both teamwork performance data as well as surveys to gauge participants' trust in their agent partners. A learning agent player is developed by training on collected game data to predict future task choices of humans and is shown to increase social utility by reducing redundant work and increasing team goal achievement rates.    The empirical results indicate that participants were able to respond appropriately to changes in an agent's behavior in terms of trust and contributions to teamwork, that positive (negative) past experience and positive (negative) reputation increase (decrease) human trust in agent teammates, and humans' trust attitudes towards human and agent teammates differ. The results also show that past experience can affect three antecedents of trust: emotional state, game expertise, and expectation. The interplay between these three elements significantly affected the influence of agent performance and reputation on human trust. The detailed findings and the recommendations derived from them in this dissertation will allow agent developers to set appropriate context for more efficient and successful ad hoc collaboration between human and agent peers in virtual teams.    This dissertation enhances our understanding of the changes in human trust in peer level agent teammates with respect to agent trustworthiness, past experience, and agent reputation. The empirical findings provide clear and significant evidence of the influence of key factors on human trust in virtual agent teammates. We also derive several guidelines for agents to follow in practice.",2015,"Hafizoglu, Feyza Merve",THES,Ph.D.
proquest_dt,1753889566,On Fundamental Performance Limits of Delay-Sensitive Wireless Communications,"Mobile traffic is expected to grow at an annual compound rate of 57% from 2014 to 2019, while among the data types that account for this growth mobile video has the highest growth rate. Since a significant portion of mobile video traffic is delay-sensitive, delay-sensitive traffic will play a critical role in future wireless communications. Consequently, future mobile wireless systems will face the dual challenge of supporting large traffic volume while providing reliable service for various kinds of delay-sensitive applications (e.g. real-time conversational video, voice-over-IP (VoIP), and online gaming). Past work on delay-sensitive wireless communications has generally overlooked physical-layer considerations such as modulation and coding scheme (MCS), probability of decoding error, and code blocklength (or coding delay) by employing oversimplified models for the physical-layer. In this dissertation we aim to bridge information theory, communication theory and queueing theory by jointly considering the queueing delay violation probability and the probability of decoding error to identify fundamental trade-offs among wireless system parameters such as MCS, code blocklength, user perceived quality of service, channel fading speed, and average signal-to-noise ratio (SNR).    Throughout this dissertation we focus on wireless communication systems where the channel state information (CSI) is available only at the receiver. We model the underlying wireless channel by a finite-state Markov chain (FSMC) where state transitions happen at each transport block (TB) transmission (i.e. TB-based FSMC). First, we focus on communication schemes without feedback and derive the dispersion of the TB-based FSMC model of the Rayleigh fading channel. The TB-based FSMC dispersion is used to characterize the maximum achievable throughput under probability of decoding error and coding delay constraints for a given modulation scheme. Second, we focus on communication schemes with one bit decision-feedback (e.g. acknowledge (ACK)). We introduce a communication scheme, namely early decoding, where the receiver determines the decoding time based on the available CSI. We characterize the maximum achievable throughput of the early decoding scheme under probability of decoding error and coding delay constraints for a given modulation scheme. Then, we derive the dispersion of parallel additive white Gaussian noise (AWGN) channels with finite discrete input alphabets (e.g. pulse amplitude modulation (PAM)). The dispersion of parallel AWGN channels is used to track the operation of incremental redundancy type hybrid automatic repeat request (IR-HARQ) over the Rayleigh fading channel through the HARQ Markov model (HARQ-MM), introduced here. We use the HARQ-MM to characterize the maximum achievable (average) throughput of IR-HARQ under probability of decoding error and coding delay constraints for a given MCS. Third, we focus on a queueing system where data packets arrive at the transmitter, wait in the queue, and are transmitted over the Rayleigh fading channel with IR-HARQ. We invoke a two-dimensional discrete-time Markov process and develop a recursive algorithm to characterize the maximum achievable (average) system throughput for a given MCS under queueing delay violation probability, and probability of decoding error constraints.",2015,"Sahin, Cenk",THES,Ph.D.
proquest_dt,1733949402,Active target localization and tracking with application to robotic environmental monitoring,"Thanks to advances in miniaturization, computing power, reliable sensors, and battery life, mobile robots are increasingly being used for a wide variety of environmental monitoring tasks. No longer confined to factory floors or controlled environments, robots for remote sensing in dangerous or hard-to-reach  environments could provide the same scalability, precision, and reliability to environmental monitoring as they did to industrial applications.  To enable this kind of long-term, reliable, autonomous mobile sensor deployment, algorithms which can ensure that the robots achieve their sensing tasks are required.    In the first part of the thesis, we study the problem of using one or more mobile robots equipped with bearing sensors to locate a stationary target in minimum time. The problem requires optimizing the measurement locations of the robots  to gather the required information about the target's location.  In addition, when multiple  robots collaborate, we include communication constraints in the path planning objective. Two formulations for this problem are studied. First, we study the offline problem of finding measurement trajectories when the true target location is known. Second, we study the online version and show how to adapt the offline solution to the situation when the target location is not known, while preserving the quality guarantees of the offline solution.    In the second part of the thesis,  we study the problem of locating multiple stationary targets using a single mobile robot. We formulate a novel coverage problem and provide two main results. We first study the problem of initializing consistent estimate  of the targets' locations.  These initial estimates are used to seed an active localization algorithm which is shown to localize the targets quickly.  In a second formulation, we assume that the targets are within a set of polygonal regions, but have no further information about the distribution or number of targets in the environment.  An algorithm is provided which can choose measurement locations to localize all the targets to within desired precision in near optimal time.    In the third part of the thesis, we study the problem of using bearing information to track and capture a moving target. We present two formulations based on  pursuit-evasion games. In the open plane, the objective is for a mobile robot to minimize the distance to a maneuvering target when only uncertain bearing information is available to the robot. Then, we study the problem of capturing the maneuvering target in a closed environment by moving close to it. We show that the size of the environment relative to the sensing noise determines if this is possible.    In addition to theoretical results, we present field studies of using one or  more mobile robots to detect radio transmitters using these results. We show that the algorithms presented are suitable for use in monitoring invasive fish.",2015,"Vander Hook, Joshua David",THES,Ph.D.
proquest_dt,1731272778,"Robot Motion Planning for Tracking and Capturing Adversarial, Cooperative and Independent Targets","The last ten years have seen robots become smaller, lighter and more maneuverable than ever before, paving the way for their use in the service sector, and even in our homes. A key aspect that elevates a network of robots over a bed of sensors is the ability of individual units to autonomously navigate, forming a sensor-actuator network. These added degrees of freedom demand that we revisit existing motion planning and navigation algorithms and extend them to be able to better contribute to the services they are designed to provide.   In deploying autonomous robots away from laboratory-controlled settings and in to real-world environments, care must be taken because they share their space with other autonomous agents. For instance, a typical home robot has to account for people as well as their pets. Imparting a fair degree of autonomy to robot navigation tasks thus requires reasoning about the mobility of other entities in the path-planning process. This is not a trivial task because such agents can not often be easily characterized or quantified, e.g. human navigation depends on many factors including the perception of hazards, experience from long-term memory, even whim.   In this thesis, we take a deep look at the role of the information available to path-planning algorithms in the context of the mobile target-tracking problem. We start with the problem formulation (Chapter 2), in which robots need to keep track of a single mobile target for as long a duration of time as possible. In the game-theoretic sense, the information available to the target has a different characteristic than the information available to the robots. For example, target behavior varies from adversarial ones in defense applications to cooperative ones in service applications. In the chapters that follow, we present the main contributions of this thesis with this role of information during the path-planning process being the common thread.   Traditional surveillance applications assume the worst-case – the target is an adversary actively trying to escape from the robots. We model this strategic interaction with the theory of pursuit-evasion games, wherein a robot pursues a target, which in turn tries to evade it. We study a mathematical variant of the Lion and Man game in the presence of an obstacle, and show that the final outcome of whether the lion can track down the man depends, in closed form, on the initial conditions (Chapter 3). We derive optimal player strategies that work regardless of what the other player does, thus providing the worst-case guarantees that such applications demand.   At the opposite end of the spectrum, there exist service applications where the target’s trajectory is known to the robots before they need to move. Our motivating example is mobile telepresence, or virtual presence – an emerging technology that enables people to participate in environments they are not physically present in. Specifically, we present a navigation algorithm for a first-of-its-kind person-following robot with telepresence and monitoring applications (Chapter 4). We require that in the presence of another person, the robot should ensure a safe distance by following that person’s path without losing track. We present a heuristic planning approach that accounts for both non-holonomic constraints and trajectory smoothness. Further, a control-theoretic implementation is provided.   Targets that navigate autonomously usually do so with their own intentions. While it is uncertain how they navigate, their behavior may neither be adversarial, nor completely known to the robots in advance. We present a novel data-driven probabilistic mobility model that can be used by path planners to reason about the uncertainties in decisions made by an individual who is navigating in an indoor environment (Chapter 6). We show that it is possible to preserve long temporal histories over an abstracted representation of the environment, which helps predict future mobility of the target better than previous approaches. We present a multi-robot planning algorithm that builds off of this model. Although our algorithm is designed for long-term planning and offline solution, we are able to execute the robot paths in real-time, and demonstrate extended utility with simulations. We discuss the architecture of a complete system implementation for the telepresence application, including both hardware design and software development (Chapter 8). With an increasing aging population in the U.S., it is our belief that such a system would become relevant in the near future, e.g., to assisted living facilities for purposes of healthcare monitoring.",2015,"Karnad, Nikhil",THES,Ph.D.
proquest_dt,1708665164,Hierarchical goal networks: Formalisms and algorithms for planning and acting,"In real-world applications of AI and automation such as in robotics, computer game playing and web-services, agents need to make decisions in unstructured environments that are open-world, dynamic and partially observable. In the AI and Robotics research communities in particular, there is much interest in equipping robots to operate with minimal human intervention in diverse scenarios such as in manufacturing plants, homes, hospitals, etc. Enabling agents to operate in these environments requires advanced planning and acting capabilities, some of which are not well supported by the current state of the art automated planning formalisms and algorithms. To address this problem, in my thesis I propose a new planning formalism that addresses some of the inadequacies in current planning frameworks, and a suite of planning and acting algorithms that operate under this planning framework.   The main contributions of this thesis are: - Hierarchical Goal Network (HGN) Planning Formalism. This planning  formalism combines aspects (and therefore harnesses advantages) of Classical Planning and Hierarchical Task Network (HTN) Planning, two of the most prominent planning formalisms currently in use. In particular, HGN planning algorithms, while retaining the efficiency and scalability advantages of HTNs, also allows incorporation of heuristics and other reasoning techniques from Classical Planning. -  Planning Algorithms. Goal Decomposition  Planner (GDP) and the Goal Decomposition with Landmarks (GoDeL) planner are two HGN planning algorithms that combines hierarchical decomposition with classical planning heuristics to outperform state-of-the-art HTN planners like SHOP and SHOP2. -  Integration with Robotics. The Combined HGN and Motion Planning (CHaMP) algorithm integrates GoDeL with low-level motion and manipulation planning algorithms in Robotics to generate plans directly executable by robots.   Given the need for autonomous agents to operate in open, dynamic and unstructured environments and the obvious need for high-level deliberation capabilities to enable intelligent behavior, the planning-and-acting systems that are developed as part of this thesis may provide unique insights into ways to realize these systems in the real world.",2015,"Shivashankar, Vikas",THES,Ph.D.
proquest_dt,1540510743,Charlie: A new robot prototype for improving communication and social skills in children with autism and a new single-point infrared sensor technique for detecting breathing and heart rate remotely,"This research delivers a new, interactive game-playing robot named CHARLIE and a novel technique for remotely detecting breathing and heart rate using a single-point, thermal infrared sensor (IR). The robot is equipped with a head and two arms, each with two degrees of freedom, and a camera. We trained a human hands classifier and used this classifier along with a standard face classifier to create two autonomous interactive games: single-player (""Imitate Me, Imitate You"") and two-player (""Pass the Pose""). Further, we developed and implemented a suite of new interactive games in which the robot is teleoperated by remote control. Each of these features has been tested and validated through a field study including eight children diagnosed with autism and speech delays. Results from that study show that significant improvements in speech and social skills can be obtained when using CHARLIE with the methodology described herein. Moreover, gains in communication and social interaction are observed to generalize from child-to-robot to co-present others through the scaffolding of communication skills with the systematic approach developed for the study.  Additionally, we present a new IR system that continuously targets the sub-nasal region of the face and measures subtle temperature changes corresponding to breathing and cardiac pulse. This research makes four novel contributions: (1) A low-cost, field-tested robot for use in autism therapy, (2) a suite of interactive robot games, (3) a hand classifier created for performing hand detection during the interactive games, and (4) an IR sensor system which remotely collects temperatures and computes breathing and heart rate. Interactive robot CHARLIE is physically designed to be aesthetically appealing to young children between three and six years of age. The hard, wood and metal robot body is covered with a bright green, fuzzy material and additional padding so that it appears toylike and soft. Additionally, several structural features were included to ensure safety during interactive play and to enhance the robustness of the robot. Because children with autism spectrum disorder (ASD) often enjoy exploring new or interesting objects with their hands, the robot must be able to withstand a moderate amount of physical manipulation without causing injury to the child or damaging the robot or its components. CHARLIE plays five distinct interactive games that are designed to be entertaining to young children, appeal to children of varying developmental ability and promote increased speech and social skill through imitation and turn-taking.  Remote breathing and heart rate detection Stress is a compounding factor in autism therapy which can inhibit progress toward specific therapeutic goals. The ability to non-invasively detect physical indicators of increasing stress, especially when they can be correlated to specific activities and measured in terms of length and frequency, can relay important metrics about the antecedents that cause stress for a particular child and can be used to help automate the evaluation of a child's progress between sessions. Further, collecting and measuring critical physiological indicators such as breathing and heart rate can enable robots to adjust their behavior based on the perceived emotional, psychological or physical state of their user. The utility and acceptance of robots can be further increased when they are able to learn typical physiological patterns and use these patterns as a baseline for identifying anomalies or possible warning signs of various problems in their human users.  We present a new technique for remotely collecting and analyzing breathing and heart rates in real time using an autonomous, low cost infrared (IR) sensor system. This is accomplished by continuously targeting a high precision IR sensor, tracking changes in the sub-nasal skin surface temperature and employing a sinusoidal curve-fitting function, Fast Fourier Transform (FFT), and Discrete Wavelet Transform (DWT) to extract the breathing and heart rate from recorded temperatures.",2013,"Boccanfuso, Laura",THES,Ph.D.
proquest_dt,1500827076,Distributed constrained Bayesian optimization: Autonomous camera control,"This dissertation describes methods to autonomously control an  intelligent camera network with changeable pan, tilt, and zoom (PTZ) parameters for the purpose of obtaining high resolution facial imagery of randomly maneuvering targets. Every camera is treated as a self interested decision making agent that works in cooperation with the other agents in the network to attain a predefined system goal. The per camera per target image quality is designed and defined mathematically to formulate a distributed constrained optimization problem. Each camera is restricted to alter its own PTZ settings. All cameras use information broadcasted by neighboring cameras such that the PTZ parameters of every camera are optimized relative to the global objective. At certain times of opportunity, due to the configuration of the targets relative to the cameras, and the fact that each camera may track many targets, the camera network may be able to reconfigure itself to achieve a required target tracking specification for each target with remaining degrees-of-freedom. The remaining degrees-of-freedom can be used to obtain high resolution facial images from desirable viewing angles for certain targets. The challenge is to design algorithms that autonomously find these time instants, the appropriate imaging camera, and the appropriate parameter settings for all cameras to capitalize on these opportunities. The methodologies and solutions proposed herein involve a Bayesian formulation. The Bayesian formulation automatically trades off objective maximization versus the risk of losing target tracking performance. The dissertation describes a mathematical formulation of the visual sensing problem, design of functions that provide a measure of system performance, development of distributed methodologies that allows cameras to exchange information and asymptotically converge on optimal solutions, and incorporation of planning into the PTZ optimization methodology. The work herein presents theoretical solutions and analyses of results obtained on a simulated network of smart PTZ cameras.",2013,"Morye, Akshay Ajit",THES,Ph.D.
proquest_dt,1492749212,Path Planning in Similar Environments,"Path planning aims at navigating a robot from an initial configuration to a goal configuration without violating various constraints. The problem of path planning is theoretically intractable (PSPACE hard), but in everyday life we (as human beings) navigate in our environment without much difficulty. This is partially due to the experiences we learned since childhood. The learning process may be complex, but one of the reasons that we can learn such tasks is that most objects we encounter today are similar or identical to the objects we encountered yesterday or even years ago. Environments with similar objects are quite common. For example, desks and chairs in a classroom or in an office may be moved around from one place to another frequently, but unfamiliar items are seldom introduced. Even different environments, such as two apartments, or a manufacturing factory and an airport garage, may share many similar items. The main differences are usually in the arrangements. Similar environments can also be found in simulated reality, e.g., in different levels of a video game or in the different regions of a virtual reality world, where many objects are intentionally duplicated to reduce the (e.g., modeling and rendering) complexity. A dynamic environment where obstacles allowed to move can be considered as a continuous sequence of similar static environments due to motion coherence. We term ""discrete similar-workspace problem"" for static environments and ""continuous similar-workspace problem"" for dynamic environments. In this thesis, I investigated path planners that can address both problems and recognize this similarity in order to significantly improve efficiency and completeness.    More specifically, I developed a path planner which exploits similarity across different static environments. This planner remembers the computation (the ""mental image"") for every obstacle encountered. Given a new problem, by carefully reusing these existing computations, the planner quickly builds a roadmap from its knowledge base. In order to improve reusability, I also designed a new shape matching method for polygons. The experimental results show that the planner is 3–8 orders of magnitude faster than most existing methods in environments with various similarity. Moreover, the success rate is kept at 100%. In other words, once a query is solved, it guarantees to be able to find a collision free path in the subsequent runs.    Environments with dynamic obstacles can also be viewed as a sequence of similar environments. In fact, existing methods have explored the temporal coherence (i.e. similarity) and gain efficiency by preserving the bulk of the roadmap and only repairing the invalid part due to motions of obstacles. However, all these methods repair blindly and periodically at fixed time intervals with little attempt to analyze the similarity or changes of the planning spaces across different time instances. This results in either redundant updates or failure to detect invalid edges and nodes. Based on the assumption that obstacles move along some known trajectories, we proposed a motion planner that detects critical moments when the topology of the free configuration space changes. The critical moments can be classified into two categories: 1) a time of contact is when the space between two configuration obstacles closes, and 2) a  time of separation is when the space opens up. As shown in the experimental results, our planner not only avoids redundant computation (about one order of magnitude faster) but also improves the chances of finding a valid path.    In many real life applications such as rescue robots and autonomous vehicles, the robot may have no prior knowledge of obstacles' motions. I developed a new geometric tool which allows the robot to determine the critical moments when the robot and obstacles can collide. Then, only at such critical moments, the robot updates its belief of the environment and re-plans if necessary. The main challenge in predicting collision stems from the assumption that obstacles' motions are unknown. To provide conservative estimation, this new tool models all obstacles as adversarial agents so that obstacles will move in the way to minimize the time that the robot remains collision-free.",2013,"Lu, Yanyan",THES,Ph.D.
proquest_dt,1461396137,Interactive Learning for Sequential Decisions and Predictions,"Sequential prediction problems arise commonly in many areas of robotics and information processing: e.g., predicting a sequence of actions over time to achieve a goal in a control task, interpreting an image through a sequence of local image patch classifications, or translating speech to text through an iterative decoding procedure.    Learning predictors that can reliably perform such sequential tasks is challenging. Specifically, as predictions influence future inputs in the sequence, the data-generation process and executed predictor are inextricably intertwined. This can often lead to a significant mismatch between the distribution of examples observed during training (induced by the predictor used to generate training instances) and test executions (induced by the learned predictor). As a result, naively applying standard supervised learning methods—that assume independently and identically distributed training and test examples—often leads to poor test performance and compounding errors: inaccurate predictions lead to untrained situations where more errors are inevitable.    This thesis proposes general iterative learning procedures that leverage interactions between the learner and teacher to provably learn good predictors for sequential prediction tasks. Through repeated interactions, our approaches can efficiently learn predictors that are robust to their own errors and predict accurately during test executions. Our main approach uses existing no-regret online learning methods to provide strong generalization guarantees on test performance.    We demonstrate how to apply our main approach in various sequential prediction settings: imitation learning, model-free reinforcement learning, system identification, structured prediction and submodular list predictions. Its efficiency and wide applicability are exhibited over a large variety of challenging learning tasks, ranging from learning video game playing agents from human players and accurate dynamic models of a simulated helicopter for controller synthesis, to learning predictors for scene understanding in computer vision, news recommendation and document summarization. We also demonstrate the applicability of our technique on a real robot, using pilot demonstrations to train an autonomous quadrotor to avoid trees seen through its onboard camera (monocular vision) when flying at low-altitude in natural forest environments.    Our results throughout show that unlike typical supervised learning tasks where examples of good behavior are sufficient to learn good predictors, interaction is a fundamental part of learning in sequential tasks. We show formally that some level of interaction is necessary, as without interaction, no learning algorithm can guarantee good performance in general.",2013,"Ross, Stephane",THES,Ph.D.
proquest_dt,1176509453,Tractable Learning of Graphical Model Structures from Data,"Probabilistic graphical models (PGMs) provide a way to represent variables (nodes) along with their conditional dependencies (edges) and therefore allow formalizing our knowledge of the interacting entities in the real world. Structure learning aims to discover the topology (and parameters) of a PGM that represents accurately a given dataset. Accuracy of representation can be measured by the likelihood that the PGM explains the observed data, which leads to maximum likelihood estimation (MLE).    From an algorithmic point of view, one challenge faced by structure learning is that the number of possible structures is super-exponential in the number of variables. From a statistical perspective, it is important to find good regularizers in order to avoid over-fitting and to achieve better generalization performance. Regularizers aim to reduce the complexity of the PGM, which can be measured by its number of parameters.    First, we present three regularizers for MLE of Gaussian Markov random fields (MRFs): local constancy for datasets where variables correspond to a measurement in a manifold (silhouettes, motion trajectories, 2D and 3D images); variable selection for finding few interacting nodes from datasets with thousands of variables; and multi-task learning for a more efficient use of data which is available for multiple related tasks.  For these regularizers, we show bounds of the eigenvalues of the optimal solution, convergence of block coordinate descent optimization, and connections to the continuous quadratic knapsack problem and the quadratic trust-region problem.    Second, we focus on learning sparse discrete MRFs through MLE. In this case, computing the objective function as well as its gradient is NP-hard. We study the convergence rate of stochastic optimization of exact NP-hard objectives, for which only biased estimates of the gradient are available. We provide a convergence-rate analysis of deterministic errors and extend our analysis to biased stochastic errors.    Third, we show general results for PGMs that allow understanding MLE with regularizers on the differences of parameters (e.g. sparse structural changes, time-varying models), the generalization ability of PGMs, and the use of PGM parameters as features in classification, dimensionality reduction and clustering. To this end, we show that the log-likelihood of several PGMs is Lipschitz continuous with respect to the parameters, and derive bounds on the Kullback-Leibler divergence, expected log-likelihood and Bayes error rate.    Finally, we formalize and study the problem of learning the structure of graphical games from strictly behavioral data. We propose MLE of a generative model defined by the Nash equilibria of the game. The formulation brings out the interplay between goodness-of-fit and model complexity: good models capture the equilibrium behavior represented in the data while controlling the true number of equilibria, including those potentially unobserved. We provide a generalization bound for MLE. We discuss several optimization algorithms including convex loss minimization, sigmoidal approximations and exhaustive search. We formally prove that games in our hypothesis space have a small true number of equilibria, with high probability; thus, convex loss minimization is sound.    We present experimental results on a wide range of real-world datasets: walking video sequences, motion capture, cardiac MRI, brain fMRI, gene expression, stock prices, world weather and congressional voting.",2012,"Honorio Carrillo, Jean Fausto",THES,Ph.D.
proquest_dt,1027773421,Data-Driven Interaction Methods for Socially Assistive Robotics: Validation With Children With Autism Spectrum Disorders,"There exists a great untapped potential for the use of intelligent robots as therapeutic social partners for children. However, enabling a robot to understand social behavior, and do so while interacting with the child, is a challenging problem. Children are highly individual and thus technology used for social interaction requires recognition of a wide-range of social behavior. This argues for data-driven methods that capture the relevant range of interactions. This work addresses the challenge of designing data-driven behaviors for socially assistive robots in order to enable them to recognize and appropriately respond to a child's free-form behavior in unstructured play contexts. The focus on free-form behavior is inspired by and grounded in the DIR/Floortime approach to therapeutic intervention with children with autism spectrum disorders (ASD). This approach emphasizes fostering engagement through play, recognizing social behavior and using ""engagements"" to bolster social interactions.    This research presents a data-driven methodology and a validated experimental framework for enabling fully autonomous robots to interact with both typically developing children and children with ASD in undirected scenarios using socially appropriate behavior, especially where spatial interaction is concerned. Autonomous robot operation as a critical aspect of the methodology; save for safety interventions by a human operator, the robot acts of its own accord. The robot and child engage in free-form interaction, in part though distance-oriented behaviors; the robot must be able to recognize the child's behaviors and respond to them appropriately. This dissertation presents the following computational contributions with therapeutic potential:    • Detection and mitigation of a child's distress: This work presents methodology for learning and then applying a data-driven spatio-temporal model of social behavior based on distance-based features to automatically differentiate between typical vs. aversive child-robot interactions. Using a Gaussian Mixture Model learned over distance-based feature data, the developed system is able to detect and interpret social behavior with sufficient accuracy to recognize child distress. The robot uses this model to change its own behavior so as to encourage positive social interaction.    • Encouragement of human-human and human-robot interaction: This work demonstrates a global and local motion planner that uses the above spatio-temporal model as part of the determination of a motion trajectory that maintains the robot's spatial relationship with the child and sustains interaction while also encouraging the child to move toward another proximal interaction partner. The desired spatial interaction behavior is achieved by modifying an established trajectory planner to weight candidate trajectories based on conformity to a trained model of the desired behavior.    • Data-Driven Approach For Providing Graded Cueing Feedback: A methodology for robot behavior that provides autonomous feedback for a robot-child imitation and turn-taking game. This is accomplished by incorporating an established therapeutic model of feedback along with a trained model of imitation behavior. This is used as part of an autonomous system that can play a turn-taking game, recognize breeches in imitation behavior, and interpret a breech in order to provide appropriate feedback. The approach is validated in a spatial imitation game, used to gauge the presence of imitation behavior.    The three main contributions above: averse behavior detection, model-based trajectory planning, and data-driven feedback, have been instantiated and validated in several SAR systems using autonomous person sensing, behavior interpretation, and action selection, for the purposes of detecting, provoking, and encouraging both human-human and human-robot social interaction. The validated systems were tested in experiments that evaluated the system design, the accuracy of the robot's ability to interpret observed behavior, the appropriateness of the robot's responses, and the quality of the child-robot and child-parent social behavior interaction. The evaluation experiments were conducted with both children with ASD and typically developing children. The systems were also used to explore the therapeutic potential of socially assistive robots facilitated by the developed models, architecture, and experiment framework.",2012,"Feil-Seifer, David J.",THES,Ph.D.
proquest_dt,847028622,The interplay between networks and robotics: Networked robots and robotic routers,"In this work, we explore the interplay between robotics and networks. Robots can benefit from an embedded network and also the network can benefit from the mobility of the robots.   We investigate the design space where the application is oriented towards robotics or networking, considering how much information about the environment is provided and what the sensing capabilities are.   At one end, the robots can benefit from the resources of an environment-embedded network, in which robot sensing and communication is enhanced by the network. We consider the design and implementation of practical pursuit-evasion games with networked robots, where a communication network provides sensing-at-a-distance as well as a communication backbone that enables tighter coordination between pursuers. Using the theory of zero-sum games, we develop an algorithm that computes the minimal completion time strategy for multi-pursuit multi-evasion when all players make optimal decisions based on complete knowledge. We then describe the design of a real-world mobile robot-based pursuit evasion game. We validate our algorithms by experiments in a moderate-scale testbed in a challenging office environment.   We then show that the network can also benefit from Robotics by taking advance of micro and macro motion. Robots can mitigate multi-path fading. We design a system that allows robots to cooperate and improve the real-world network throughput via a distributed cooperation framework. A mobile wireless network can also be quickly and autonomously deployed in urban search and rescue efforts, forming a communication substrate. We study the problem of determining the minimum number of mobile robots and how to position them so all clients are connected. Our approach to the problem is based on virtual potential fields where we treat each client as a virtual charged particle. We validate our algorithm with physical robots in an indoor environment and demonstrate that we are able to get feasible solutions.",2010,"Vieira, Marcos Augusto Menezes",THES,Ph.D.
proquest_dt,305176640,Autonomic trust management in dynamic systems,"Research in pervasive computing is aimed at creating environments where users can seamlessly benefit from ubiquitous computing resources despite the complexity of the environment. Providing security in such systems is a difficult task since traditional security mechanisms often require significant user attention and do not scale well to large, mobile, and open environments. To combat this problem, distributed trust has been proposed to provide security in pervasive systems. While much research has been performed in the area, many vulnerabilities and insufficiencies still exist, especially in mobile ad-hoc systems that cannot support distributed trust mechanisms requiring pre-existing infrastructure and cooperation. Dynamic pervasive systems operate in highly dynamic environments that introduce additional challenges such as intermittent connectivity and lack of infrastructure.   This dissertation addresses several problems pertinent to the design and deployment of distributed trust mechanisms in dynamic pervasive systems. In particular, this dissertation presents the design and evaluation of the following framework and mechanisms to enhance security in dynamic systems. The Distributed Trust Toolkit (DTT) is a modular framework for the design and deployment of distributed trust mechanisms over a wide variety of systems, networks, and devices. Adaptive Resource Exploration (AREX) and Reliable Service Composition (ReSCo) are built for two specific classes of applications that occur frequently in dynamic systems. AREX uses a game theoretic approach to motivate strategic, malicious entities to attack less often. ReSCo is designed for dynamic service composition systems and works by adapting to make selections of compositions paths and nodes. Social Trust (SoTru) is a system for augmenting trust mechanisms such as AREX and ReSCo with information from users' social networks to reduce risk and enhance their performance. A unique feature of the above contributions is that each can be used independently or in combination to address challenges in secure dynamic systems. DTT facilitates the integration of AREX, ReSCo and SoTru into existing dynamic systems. AREX and ReSCo provide scalable, low cost security mechanisms that provide protection despite hostile, open, and mobile environments. When used together, with the addition of SoTru, the ideas presented in this dissertation can be used to enhance the effectiveness and seamlessness of security in dynamic systems.",2009,"Lagesse, Brent Jason",THES,Ph.D.
proquest_dt,305084774,Economic incentive mechanisms for user-contributed wireless networks,"User-contributed wireless networks are formed by wireless devices that are owned by different users. They have been widely used to achieve better connectivity at places where an infrastructure is not immediately available or cannot be directly used. The functioning of the user-contributed wireless network depends on the cooperation of the nodes in the network. In civilian wireless networks, nodes often belong to different users who have their own interests and always want to maximize their benefits. Consequently, the user nodes may not want to behave cooperatively, if it does not suit their needs and interests. Like many distributed autonomous systems, user-contributed wireless networks have the common incentive problems such as the free-rider problem, where only a small portion of user nodes contribute their resources, and the adverse selection problem, where user nodes do not reveal their states truthfully. So, to ensure the functioning of user-contributed wireless networks, it is highly important to provide incentives for nodes to cooperate. Game theory is a nature tool to deal with the problem of selfish behavior. In our work, we adopt solution concepts from game theory to study important incentive problems in user-contributed wireless networks. We mainly focus on two categories of incentive problems: routing problems and spectrum sharing problems. For routing, we have done four works. First, we considered the traditional deterministic routing in user-contributed wireless ad hoc networks, and proposed a mechanism to prevent the user nodes from colluding. Second, we studied the incentive issues for opportunistic routing in user-contributed wireless mesh networks, and proposed mechanisms to stimulate nodes to truthfully reveal their link states, so that the most efficient routing decision can be reached. Third, we proposed the first bargaining-based incentive mechanism for probabilistic routing that stimulates selfish nodes to participate in message forwarding. Fourth, we proposed a strategy-proof mechanism to efficiently distribute data flows among node-disjoint paths obtained by multipath routing protocols. For spectrum sharing, we studied multi-radio multi-channel assignment problem in a single collision domain, and proposed an incentive mechanism to ensure the system converge to equilibrium, named strongly dominant strategy equilibrium. At the same time, this equilibrium state achieves optimal system throughput.",2009,"Wu, Fan",THES,Ph.D.
proquest_dt,305016982,Incentive-centered design for scheduling in parallel and distributed systems,"Distributed systems comprise components that are owned and operated by autonomous organizations. These organizations are rational (self-interested and welfare-maximizing) and they have no a prior motivation for cooperation. Rationality leads them to manipulate the systems if it benefits them to do so. Organizations respond to incentives. To prevent manipulations, incentives must be explicitly aligned with the system's objectives. Incentive-centered design (ICD), which integrates the motivational human behavior aspect of social sciences with the computational tractability aspect of computer science, specifically addresses these concerns and provides methods to mitigate them.   We examine the incentives present in several distributed system resource allocation problems. We model these problems as games and then used ICD to devise protocols that obtained their desired objectives in a self-interested, competitive setting. The first problems we examine are designing incentive-compatible mechanisms for scheduling divisible loads in distributed systems. Divisible loads is a form of data parallelism in which large data sets are partitioned into fractions. All fractions require an identical type of processing. Scheduling requires knowledge of the processors. Since the processors are rational, they may choose to misreport their capacities, which adversely affects the quality of scheduling. We design several mechanisms that obtain optimal schedules in this setting. Each mechanism is specific to a network architecture as the architecture affects partitioning and the optimality conditions. We examine three architectures: bus, linear, and tree networks. For the bus network, we design three classical centralized mechanisms. The centralized mechanisms are not feasible for the tree and linear networks as the processors cannot communicate directly with the mechanism center, a limitation of these architectures. Instead, we design distributed mechanisms in which the information and algorithms are distributed throughout the network. These mechanisms introduce new avenues for manipulation and we address this by creating incentives for processors to monitor one another. We then examine divisible load scheduling using coalitional games. An application owner receives a payoff when her job is finished. The amount she receives though decreases with time and furthermore, the payoff must be shared with the processors computing her job. She forms a coalition with a set of processors to maximize her profit, which is the difference between payoff and costs. We model this problem as a coalitional game and we show that stable coalitions between application users and processors occur.   We next examine parallel job scheduling in parallel systems comprising identical processors. A parallel job differs from a sequential job or task in that it can execute on two or more processors. We consider two different types of parallel job scheduling. The first type we examine is the batch scheduling of rigid jobs. Batch scheduling requires that all jobs are present at the start of scheduling. A parallel job is rigid if it requires a fixed number of processors. If fewer processors are assigned, the job fails. If more processors are assigned, the job does not speedup. Associated with each job is a deadline and payoff. The job's owner receives the payoff if her job completes by its deadline; otherwise, her payoff is zero. The scheduling objective then is to maximize the sum of payoffs. The users are rational: they will misreport their jobs' parameters if it benefits them to do so. We design an incentive-compatible scheduling mechanism aimed specifically for this problem. Because the mechanism is incentive compatible, rational users choose to truthfully declare their parameters to the scheduler. The second type of parallel job scheduling we consider is the online scheduling of malleable jobs. Online scheduling differs from the batch scheduling in that the scheduler cannot foresee jobs arriving in advance. The scheduling objective must be optimized with the lack of information. Unlike rigid jobs, a malleable jobs can be adapted to execute on any number of processors. Each job is associated with an arrival time (release time), deadline, and payoff. Similar to the rigid job problem, if the job completes by its deadline the self-interested user receives the payoff, otherwise, she receives zero. Besides the online aspect, this problem differs from the rigid one in that the scheduler must choose the number of processors to assign to the job. For this problem we design an online, incentive-compatible mechanism. If a job is scheduled to execute, the mechanism assigns it the number of processors that minimizes its execution time.",2009,"Carroll, Thomas E.",THES,Ph.D.
proquest_dt,304966110,Decentralized network design,"Most overlay networks are designed by a centralized algorithm, which takes the participating nodes and returns a set of edges connecting the nodes. We study the possible results of decentralizing overlay network design. What would happen if the individual nodes in the network were asked to choose their own connections? We use techniques from game theory to consider issues such as the stability and fairness of the resulting networks. We define and study several games, all with relevant applications.   First, we study variants of the Bounded Budget Connection (BBC) game, in which each node has some budget to spend on outgoing links and wants to minimize its distance to other nodes in the network. The BBC game has applications in social networks as well as peer-to-peer and overlay networks. We show that this game does not always guarantee a stable network. However, if we allow nodes to fractionally purchase links, as in the fractional BBC game, then a stable solution always exists, although it may be hard to find. We give existence and hardness results for fractional BBC games, as well as for two other fractional games: the fractional Shortest Paths Problem (SPP) game, which is motivated by the routing behavior of Autonomous Systems on the internet using the Border Gateway Protocol, and the preference game, a very simple fractional game which is a specialization of both fractional BBC and fractional SPP games. We study a new equilibrium solution concept for matrix games, called personalized equilibria, which generalizes all of these fractional games as well as modeling scenarios where players can choose how to combine other players' actions to suit their individual interests. Finally, we study the interaction between a centralized algorithm placing resources in the network and nodes choosing their own edges or routes – a variant of multicommodity facility location in which clients pay the cost of a minimum group Steiner tree connecting them to commodities of interest.",2009,"Ma, Laura Jane Poplawski",THES,Ph.D.
proquest_dt,304918628,Motivation and discourse in a literate environment: A case study of a Young Adult library,"This case study of the Young Adult section of a public library employed the Self-Determination Theory and discourse analysis to determine the elements of the social context that contribute to adolescents’ motivation to read and write, and how the needs for autonomy, competence, and relatedness are met as expressed through three adolescents’ literate identities. The two key elements for motivation were found—access to preferred genre and social interaction--while the adolescents demonstrated different needs satisfaction independently and in relationship to the Young Adult section and school contexts. Adolescents’ use of online text was dominated by three types of text—social networking, gaming, and other sites. Teens were found to use the text in their live discourse in five ways: reference, authority, experience, expression, and instrument. This discourse at the computers built friendships and explored sexual identities. The author questions whether incorporation of out-of-school literacies is possible on a broad scale if they are to maintain their attached discourses and meet the teens’ expectations to be prepared for the workplace.",2009,"Berg, Margaret A.",THES,Ph.D.
proquest_dt,304864473,Modeling behavior and variation for crowd animation,"The simulation of crowds of virtual characters is needed for applications such as films, games, and virtual reality environments. These simulations are difficult due to the large number of characters to be simulated and the requirement for synthesizing realistic human-like motion efficiently. This thesis focuses on two problems: how to search through and select motion clips of behaviors so that human-like motion can be generated for multiple characters interactively, and how to model and synthesize variation in motion data.    Given a collection of blendable segmented motion clips derived from motion capture or keyframed animation, this thesis explores novel ways to apply heuristic search algorithms to generate goal-driven navigation motion for virtual humanlike characters. Motion clips are organized and interconnected through a behavior graph that encodes the possible actions of a character. A planning approach is used to search over these possible actions to efficiently generate motion. This technique works well for synthesizing animations of multiple characters navigating autonomously in large dynamic environments.    In addition, this thesis introduces a novel planning approach based on precomputation that is more efficient than traditional forward search methods. We present a technique for precomputing large and diverse trees, and describe a backward search method used during runtime to solve planning queries. This new approach allows us to develop an interactive animation system that supports a large number of characters simultaneously.    Finally, this thesis addresses the issue of motion variation. Current state-of-the-art crowd simulations often use a few specific motion clips or repeated cycles of a particular motion to continuously animate multiple characters. The idea of synthesizing the subtle variations in motion data has been largely unexplored, as previous work considers variation to be an additive noise component. This thesis instead uses a data-driven approach and applies learning techniques to this problem. Given a small number of input motions, we model the data with a Dynamic Bayesian Network, and synthesize new spatial and temporal variants that are statistically similar to the inputs.",2009,"Lau, Manfred Chung Man",THES,Ph.D.
proquest_dt,33910975,Service assurance in insecure networks with Byzantine adversaries .,"This dissertation describes research into security threats in new communication environments and methods to counter them. More specifically, we consider networks where some nodes perform an important application function such as routing, filtering, aggregation, etc and may become compromised while doing so. We consider three types of networks: Internet-scale publish/scribe networks, aggregating sensor networks, and peer-to-peer massively multiplayer online games (MMOG) and virtual environments. These environments are complementary to each other in many respects. A publish/scribe network is responsible for disseminating data objects produced by one set of nodes (publishers) to another set of nodes (subscribers). Subscribers want to receive those and only those objects that satisfy their interests. A large-scale publish/scribe network using many network providers, each under its own administrative control, presents numerous opportunities for mischief. In many cases the network providers will not trust one another and will not be fully trusted by the end users. A malicious network provider may insert, delete, modify, reorder, misdirect, or delay messages, and remain undetected. The first topic of our research is how to assure service integrity in such networks if some of the intermediate nodes may attack the system in an arbitrary fashion. We solve the problem by creating filtering agents corresponding to user subscriptions, and mapping these agents to either hosts from trusted providers or to clusters of hosts taken from multiple non-trusted providers. As a whole, each cluster can be trusted since only a relatively small number of providers are assumed to be malicious. A sensor network is a network connecting hundreds or thousands of sensors, tiny battery-powered computers equipped with units measuring some physical phenomena (e.g light intensity, temperature, humidity, ambient chemical composition, etc.) and wireless radio transceivers. We study aggregating sensor networks that do not fully propagate raw measurements to their users but rather perform in-network aggregation with the intent of lowering the total amount of transmitted data thus preserving bandwidth and energy. As sensor networks are frequently placed in hostile environments (for instance, in military applications) it is important to devise mechanisms guaranteeing integrity of their service under attack, and their survivability. In this dissertation we discuss CoDeX, a collect-detect-exclude framework for secure aggregation in sensor networks. Our approach to solving the problem is based on the fact that many physical phenomena exhibit strong spatial correlation. Sensor nodes can take advantage of the broadcast nature of radio transmissions, receive measurements from their neighbors, and compare them with their own results. If the values significantly differ, the fact can be reported to the user (the 'collect' phase). If a node is a subject of many such reports, it is, probably, compromised (the 'detect' phase) and should be removed from the network (the 'exclude' phase). We complement this approach with the use of randomized delivery (aggregation) trees, cryptography, and repeated aggregation of the same data in different configurations. Peer-to-peer massively multiplayer online games and virtual environments pose challenges similar to those in the other two types of networks: all three lack centralized control and run autonomously. Like pub-sub networks, P2P-based MMOGs may span the Internet and contain thousands and, potentially, hunderds of thousands of nodes. Like sensor networks, these systems almost completely rely on end nodes for providing infrastructure services. Unfortunately, most games do not provide sufficient safeguards against cheating and fraud perpetrated by the players. We developed FRAPPE, an architecture that significantly reduces this vulnerability by forming trusted 'supernodes' out of non-trusted peer machines, employing authentication, confidentiality and pseudonymity services, using secret sharing and other secure multiparty computation techniques, and constructing anonymizing tunnels to hide identities of communicating parties. We also introduce a useful primitive, called local broadcast with verification (LBV), used to solicit services of peers in a particular neighborhood and subsequently verify that the peers properly executed the protocol. Using a combination of analysis and experimental results we demonstrate that all three approaches provide strong service assurance guarantees for their respective types of networks.",2008,"Rabinovich, Paul",THES,NA
proquest_dt,33769718,MAC aware routing protocols in wireless ad hoc networks.,"This thesis studies efficient load-aware and multi-path routing for wireless ad hoc networks. Due to the lack of fixed infrastructure and centralized management, ad hoc networks need to be autonomous operating and self-organizing. These operations mainly rely on highly distributed and dynamic routing protocols. However, developing ad hoc routing protocols is challenging for reasons such as dynamic topologies, resource constraints and the broadcasting nature of wireless medium and directional transceiver. We present routing protocols for ad hoc networks that use Medium Access Control (MAC) information as indicators for making routing decisions. Traffic load information of ad hoc networks is investigated and a delay-based load metric that includes MAC channel contention information is used in the load aware routing. Ad hoc networks with directional antenna are also investigated from the perspective of routing scheme. A routing protocol based on transmit direction instead of next-hop is proposed to take advantage of the directional feature. This thesis also presents a game theoretic approach to the routing of multiple real-time multimedia streams through multiple shared links with different link quality metrics.",2008,"Li, Yang",THES,NA
proquest_dt,33473591,Rain in vision and graphics.,"Rain produces sharp intensity fluctuations in images and videos which severely degrade the performance of outdoor vision systems. Removing the visual effects of rain is therefore important in order to make outdoor vision robust to rain. In contrast, in graphics, rain effects are desirable as they are often used to convey scene emotions in movies and in other graphics applications, such as games, to enhance realism. Developing efficient algorithms for realistic rendering of rain effects is however challenging. This thesis, studies rain from the perspective of vision and graphics. We have two main goals - (1)to understand the visual appearance of rain and (2)to develop efficient algorithms both for handling its effects in vision and for its realistic rendering in graphics. We begin by modeling the visual appearance of rain. We describe the appearance by modeling the photometric intensities produced by individual raindrops and the space-time correlation produced by the motion of a large number of rain drops. We then use these appearance models to develop two complementary approaches to handle the effects of rain on vision systems. The first approach uses a post-processing algorithm for detection and removal of rain from videos that have already been captured. In the second approach we use our appearance model to show that the unique physical properties of rain--its small size, high velocity and spatial distribution--makes its visibility depend strongly on camera parameters. This dependence is used to reduce the visibility of rain during image acquisition by judiciously selecting camera parameters. Conversely, camera parameters can also be chosen to enhance the visibility of rain. This ability can be used to develop an inexpensive and portable camera-based rain gauge that provides instantaneous rain-rate measurements. In graphics, we have developed two models for realistic rendering of rain. In the first model we consider the complex appearance patterns produced by close by raindrops. We show that these patterns are produced due to the rapid shape distortions (i.e. oscillations) that a raindrop undergoes as it falls and develop a physics-based model to faithfully render these complex appearance patterns. This model is then used to develop an efficient algorithm for rendering rain in images and videos. In the second work, we focus on realistic rendering of material-dependent splashing of raindrops. Here we measure raindrop splashes for 22 different realworld materials. These measurements are then used to build a compact stochastic model that accurately captures the material and inclination dependent splashing behavior of raindrops. Additionally, it allows a user to render physically plausible splashes for novel materials that have not been measured. These two methods have advanced the state of the art algorithms for rendering rain. We believe that this thesis presents a new approach for modeling dynamic weather effects and has opened new avenues for research in vision and graphics.",2008,"Garg, Kshitiz",THES,NA
proquest_dt,304812003,The large scale peer-to-peer (P2P) live streaming in the Internet,"A large number of emerging applications such as IPTV, event broadcast, online games and distance learning require the support of live video streaming, yet, this is perhaps the greatest unfulfilled promise of the Internet. The root of the problem is that the Internet by nature, i.e., autonomous, heterogeneous, best-effort, can not provide the services required by streaming applications. The recent development in Peer-to-Peer (P2P) technologies brings new momentum in live video streaming due to the inherent self-scaling property and easy deployment. In spite of its popularity, there is no consensus on how a large-scale P2P live streaming system works. There are two fundamental problems in the design space: topology formulation that relates to how a peer locates the video content from one another and content delivery. Further, there has been little study on the design trade-offs and large-scale measurements. This thesis fills this gap.    We leverage our earlier system, Coolstreaming, which was arguably the earliest large-scale P2P video streaming experiment and was widely referenced in the community as the benchmark (Google entries top 400,000). We have designed and implemented comprehensive logging tools to collect and analyze large sets of traces from real-world broadcasts, from which we establish a theoretical framework that (1) concretely demonstrate the fundamental system design trade-offs and further identify the main performance bottlenecks and key factors behind them. Specifically, we show (1) random topology formulation can lead to convergence and stability; (2) video streaming performance is critically affected by system dynamics, in particular churns; (3) the system exhibits excellent scaling property yet the uploading capacity contributions from peers are highly skewed, in which a small percentage of peers conbtribute most; (4) the scale and streaming performance is largely determined by how well the system can handle the flash crowd in live streaming event.",2008,"Xie, Susu",THES,Ph.D.
proquest_dt,304655498,Real-time smooth surface construction on the graphics processing unit,"Increased realism in interactive graphics and gaming requires complex smooth surfaces to be rendered at ever higher frame rates. In particular, representations used to model surfaces offline, such as spline and subdivision surfaces, have to be modified or reorganized to allow for efficient usage of the graphics processing unit and its SIMD (Single Instruction, Multiple Data) parallelism. This dissertation presents a novel algorithm for converting quad meshes on the GPU to smooth, water-tight surfaces at the highest speed documented so far. The conversion reproduces bi-cubic splines wherever possible and closely mimics the shape of the Catmull-Clark subdivision surface by c-patches where a vertex has a valence different from 4. The smooth surface is piecewise polynomial and has well-defined normals everywhere.",2008,"Ni, Tianyun",THES,Ph.D.
proquest_dt,304653320,Routing in random ad-hoc networks: Provably better than worst-case,"Wireless networks, such as sensor networks or general ad-hoc networks, present special challenges for routing problems due to high energy constraints, failures, and node autonomy. As such, traditional methods of constructing and fixing a centralized routing topology and then simply expecting nodes to forward accordingly in the absence of any further incentives to do so suffer reliability drawbacks. We look to light-weight and game-theoretic proposals based on random walks and game-theoretic techniques to deal with these reliability concerns. In particular, we consider random walks for information collection, path auctions for one-to-one routing, and a locally minimum cost forwarding game (LMCF) for all-to-one reverse multicast routing. For each of the three routing scenarios considered, a trade-off between reliability and efficiency is observed in the worst-cases: There exist wireless network configurations for which random walks take a maximally long expected time to visit all nodes, path auctions require maximal overpayments to ensure truthfulness, and any Nash equilibria for locally minimum cost forwarding are arbitrarily costlier than the global optimum. We further demonstrate NP-hardness results and approximation hardness results regarding computation of optimal truthful mechanisms and optimal Nash equilibria for the game-theoretic scenarios present. On the other hand, a clique is also a possible wireless network configuration formed by setting a large enough (broadcast radius, and the clique configuration demonstrates optimal efficiency for all three. The extreme downside to such a configuration, however, is that such a large broadcast radius would rapidly deplete the system's energy, yielding it completely infeasible. What becomes apparent then, is that the appropriate question to ask is the following: What is the efficiency of each of the three proposed methods for a typical wireless network of low broadcast radius'? Does a nicer structure tend to arise despite negative worst-case behavior for the reliable routing problems considered in this work on the relevant random models of wireless networks'? A central theme of this work is a resounding, theoretically sound ""Yes"" to that question for all three scenarios: We prove that each of the three proposed methods are almost always efficient under relevant, models and metrics models for wireless networks.",2008,"Ercal-Ozkaya, Gunes",THES,Ph.D.
proquest_dt,304653149,Rain in vision and graphics,"Rain produces sharp intensity fluctuations in images and videos which severely degrade the performance of outdoor vision systems. Removing the visual effects of rain is therefore important in order to make outdoor vision robust to rain. In contrast, in graphics, rain effects are desirable as they are often used to convey scene emotions in movies and in other graphics applications, such as games, to enhance realism. Developing efficient algorithms for realistic rendering of rain effects is however challenging. This thesis, studies rain from the perspective of vision and graphics. We have two main goals - (1) to understand the visual appearance of rain and (2) to develop efficient algorithms both for handling its effects in vision and for its realistic rendering in graphics.    We begin by modeling the visual appearance of rain. We describe the appearance by modeling the photometric intensities produced by individual raindrops and the space-time correlation produced by the motion of a large number of rain drops. We then use these appearance models to develop two complementary approaches to handle the effects of rain on vision systems. The first approach uses a post-processing algorithm for detection and removal of rain from videos that have already been captured. In the second approach we use our appearance model to show that the unique physical properties of rain—its small size, high velocity and spatial distribution—makes its visibility depend strongly on camera parameters. This dependence is used to reduce the visibility of rain during image acquisition by judiciously selecting camera parameters. Conversely, camera parameters can also be chosen to enhance the visibility of rain. This ability can be used to develop an inexpensive and portable camera-based rain gauge that provides instantaneous rain-rate measurements.    In graphics, we have developed two models for realistic rendering of rain. In the first model we consider the complex appearance patterns produced by close by raindrops. We show that these patterns are produced due to the rapid shape distortions (i.e. oscillations) that a raindrop undergoes as it falls and develop a physics-based model to faithfully render these complex appearance patterns. This model is then used to develop an efficient algorithm for rendering rain in images and videos. In the second work, we focus on realistic rendering of material-dependent splashing of raindrops. Here we measure raindrop splashes for 22 different realworld materials. These measurements are then used to build a compact stochastic model that accurately captures the material and inclination dependent splashing behavior of raindrops. Additionally, it allows a user to render physically plausible splashes for novel materials that have not been measured. These two methods have advanced the state of the art algorithms for rendering rain. We believe that this thesis presents a new approach for modeling dynamic weather effects and has opened new avenues for research in vision and graphics.",2008,"Garg, Kshitiz",THES,Ph.D.
proquest_dt,304653068,Probabilistic distributed control,"Algorithms must not only allow autonomous entities to work together to accomplish some desired task, but it is often advantageous to must do so in a distributed manner. The latter criterion is critical. As the number of autonomous entities increases, it becomes increasingly unwieldy to coordinate them with a centralized controller due to scalability issues.    The natural question is thus ""What distributed control algorithm can we use for this purpose?"" As it turns out, in 1964 the mathematician M.L. Tsetlin devised a very elegant solution, which he called the Gur Game, to bring about ""optimality"" in a population of finite state automata without using a centralized controller. What is remarkable about his methodology is that it so closely parallels what we are trying to accomplish now with networks of computing entities. It is because of this that we have chosen to further investigate how the Gur Game can realize distributed control. Since the Gur Game works in a probabilistic manner, we will use the phrase ""probabilistic distributed control"" to describe this work. Indeed, it is the thesis of this dissertation that the Gur Game is an effective means to achieve probabilistic distributed control in many network applications.    In this dissertation, our first application of the Gur Game is to the problem of adjusting the resolution of active entities in ad hoc networks, such as sensor networks. We next present an extension of the Gur Game that we call the Hierarchical Gur Game. This Hierarchical Gur Game addresses some scalability issues associated with the baseline (flat) Gur Game. We next study how the Gur Game can be used in the domain of power control for ad hoc networks. Next, we examine the concept of the Gur Game duty cycle. Following that we explore what happens when we break the original Gur Game into separate, smaller games. We also demonstrate how allowing gurs to be mobile helps provide robustness in a tree hierarchy network. Finally, we analyze the Gur Game theoretically.",2008,"Iyer, Ranjit",THES,Ph.D.
proquest_dt,304575864,Mechanism design and analysis using simulation-based game models,"As agent technology matures, it becomes easier to envision electronic marketplaces teeming with autonomous agents. Since agents are explicitly programmed to optimally compete in these marketplaces (within bounds of computational tractability), and markets themselves are designed with specific objectives in mind, tools are necessary for systematic analyses of strategic interactions among autonomous agents. While traditional game-theoretic approaches to the analysis of multi-agent systems can provide much insight, they are often inadequate, as they rely heavily on analytic tractability of the problem at hand; however, even mildly realistic models of electronic marketplaces contain enough complexity to render a fully analytic approach hopeless.    To address questions not amenable to traditional theoretical approaches, I develop methods that allow systematic computational analysis of game-theoretic models in which the players' payoff functions are represented using simulations (i.e., simulation-based games). I develop a globally convergent algorithm for Nash equilibrium approximation in infinite simulation-based games, which I instantiate in the context of infinite games of incomplete information. Additionally, I use statistical learning techniques to improve the quality of Nash equilibrium approximation based on data collected from a game simulator. I also derive probabilistic confidence bounds and present convergence results about solutions of finite games modeled using simulations. The former allow an analyst to make statistically-founded statements about results based on game-theoretic simulations, while the latter provide formal justification for approximating game-theoretic solutions using simulation experiments. To address the broader mechanism design problem, I introduce an iterative algorithm for search in the design space, which requires a game solver as a subroutine. As a result, I enable computational mechanism design using simulation-based models of games by availing the designer of a set of solution tools geared specifically towards games modeled using simulations.     I apply the developed computational techniques to analyze strategic procurement and answer design questions in a supply-chain simulation, as well as to analyze dynamic bidding strategies in sponsored search auctions. Indeed, the techniques I develop have broad potential applicability beyond electronic marketplaces; they are geared towards any system that features competing strategic players who respond to incentives in a way that can be reasonably predicted via a game-theoretic analysis.",2008,"Vorobeychik, Yevgeniy",THES,Ph.D.
proquest_dt,304564235,Synthesis of strategies for non-zero-sum repeated games,"There are numerous applications that involve two or more self-interested autonomous agents that repeatedly interact with each other in order to achieve a goal or maximize their utilities. This dissertation focuses on the problem of how to identify and exploit useful structures in agents’ behavior for the construction of good strategies for agents in multi-agent environments, particularly non-zero-sum repeated games.   This dissertation makes four contributions to the study of this problem. First, this thesis describes a way to take a set of interaction traces produced by different pairs of players in a two-player repeated game, and then find the best way to combine them into a strategy. The strategy can then be incorporated into an existing agent, as an enhancement of the agent's original strategy. In cross-validated experiments involving 126 agents for the Iterated Prisoner's Dilemma, Iterated Chicken Game, and Iterated Battle of the Sexes, my technique was able to make improvement to the performance of nearly all of the agents. Second, this thesis investigates the issue of uncertainty about goals when a goal-based agent situated in a nondeterministic environment. The results of this investigation include the necessary and sufficiency conditions for such guarantee, and an algorithm for synthesizing a strategy from interaction traces that maximizes the probability of success of an agent even when no strategy can assure the success of the agent. Third, this thesis introduces a technique, Symbolic Noise Detection (SND), for detecting noise (i.e., mistakes or miscommunications) among agents in repeated games. The idea is that if we can build a model of the other agent’s behavior, we can use this model to detect and correct actions that have been affected by noise. In the 20th Anniversary Iterated Prisoner’s Dilemma competition, the SND agent placed third in the “noise” category, and was the best performer among programs that had no ""slave"" programs feeding points to them. Fourth, the thesis presents a generalization of SND that can be wrapped around any existing strategy. Finally, the thesis includes a general framework for synthesizing strategies from experience for repeated games in both noisy and noisy-free environments.",2008,"Au, Tsz-Chiu",THES,Ph.D.
proquest_dt,304463399,Procedural animation of emotionally expressive gaze shifts in virtual embodied characters,"Believably animated virtual human characters appear in many diverse fields of computer science research and areas of the technology industry, including video games, animated films, and virtual training environments. In these applications, animated virtual humans play roles ranging from friendly companions, to helpful tutors, to vicious villains. A key aspect that distinguishes these applications is user interaction. In video games and virtual training environments, interaction between virtual human characters and human users is required, in contrast to animated films. However, the animation methods that produce often very believable behavior for these animated films do not apply well to interactive domains. This leads to an expressivity gap between those animated virtual humans which are interactive and those which are not.    One ability that non-interactive animated characters possess and interactive characters do not is the ability to reveal important information about their emotional state through the use of glances or glares while the character remains silent. The goal of this research is to provide this ability to interactive characters. It does so through a gaze model capable of displaying a desired selection of physical behaviors while directing gaze towards an arbitrary target. This model of gaze, called the Expressive Gaze Model (EGM), combines body movement based on motion capture data with eye movement based on visual neuroscience data.    In addition, this research provides an empirically determined preliminary mapping between gaze behavior and emotional attribution. The results demonstrate that by obtaining a set of low-level gaze behaviors annotated with emotional data, and then generating gaze shifts through the composition of these low-level behaviors, the attribution of emotion to the resulting gaze shift can be predicted. This indicates that gaze shifts that display emotion states can be generated from these low-level gaze behaviors without using motion capture of the displayed emotion.",2008,"Lance, Brent Jason",THES,Ph.D.
proquest_dt,230863571,Edge indexing in a grid for highly dynamic virtual environments,"Newly emerging game-based application systems provide three-dimensional virtual environments where multiple users interact with each other in real-time. They are filled with autonomous, mutable virtual content which is continuously augmented by the users. To make the systems highly scalable and dynamically extensible, they are usually built on a client-server based grid subspace division where the virtual worlds are partitioned into manageable sub-worlds. In each sub-world, the user continuously receives relevant geometry updates of moving objects from remotely connected servers and renders them according to her viewpoint, rather than retrieving them from a local storage medium.    In such systems, the determination of the set of objects that are visible from a user's viewpoint is one of the primary factors that affect server throughput and scalability. Specifically, performing real-time visibility tests in extremely dynamic virtual environments is a very challenging task as millions of objects and sub-millions of active users are moving and interacting. We recognize that the described challenges are closely related to a spatial database problem, and hence we map the moving geometry objects in the virtual space to a set of multi-dimensional objects in a spatial database while modeling each avatar both as a spatial object and a moving query. Unfortunately, existing spatial indexing methods are unsuitable for this kind of new environments.    The main goal of this study is to present an efficient spatial index structure that minimizes unexpected object popping and supports highly scalable real-time visibility determination. We then uncover many useful properties of this structure and compare the index structure with various spatial indexing methods in terms of query quality, system throughput, and resource utilization. We expect our approach to lay the groundwork for next-generation virtual frameworks that may merge into existing web-based services in the near future.",2008,"Seo, Beomjoo",THES,Ph.D.
proquest_dt,33938963,Automated negotiations among autonomous agents in negotiation networks.,"Distributed software systems are a norm in today's computing environment. These systems typically comprise of many autonomous components that interact with each other and negotiate to accomplish joint tasks. Today, we can integrate potentially disparate components such that they act coherently by coordinating their actions via message exchanges. Once this integration issue is resolved, the next big challenge in computing is the automation of the negotiation process between the various system components. In this dissertation, we address this automated negotiation problem in environments where there is a conflict of interest among the system components. We present our negotiation model - a negotiation network - where a software system is a network of agents representing individual components in the system. We analyze the software system as a characteristic form game, one of many concepts in this dissertation borrowed from game theory. The agents in our model preserve the self-interest of the components they represent (their owners), and make decisions that maximize the expected utilities of their owners. These agents accomplish joint tasks by forming coalitions. We show that the problem of computing the optimal solution, where the utilities of all agents are maximized, is hyper-exponential in complexity. We present an approximate algorithm for this hard problem, and evaluate it empirically. The simulation results show that our algorithm has many desirable properties - it is distributed, efficient, stable, scalable, and simple. Our algorithm produces the optimal (social welfare maximizing) solution for 96% of cases, generates maximal global revenue for 97% of cases, converges to 90% of the best found allocation after only 10 rounds of negotiation, and finds a core-stable solution for revenue distribution among the agents for cases with nonempty core. Finally, to ensure stability for all cases, we present a sliding-window algorithm that computes the nucleolus-stable solution under all situations.",2007,"Goradia, Hrishikesh J",THES,NA
proquest_dt,33791611,Emotion modelling with human belief revision in computer games.,"Emotion modelling is receiving more and more attention from various fields, e.g. cognitive science, psychology, computer science and neuroscience. Most of these fields share the common research consensus that emotion can be beneficial to human's mental activities. This thesis is also grounded on the same consensus and makes further validations based on the following two hypotheses: One is emotional agents in games should behave more like human beings than emotionless agents; the other is that agents having full emotional architecture should obtain better playing performance than agents with only partial architecture. Based on theoretical support, the author further hypothesizes that peoples' long term belief can be one of the sources to release complex emotions. The experiment result suggests the emotional agents did perform significantly better than emotionless ones, but it was unable to significantly reflect the advantages from fully structured emotional agents over the ones of the partial architecture.",2007,"Ma, Yan",THES,NA
proquest_dt,33710735,'U Got 2 Move It' pilot study: Impact of an after-school interactive video exertainment program for underserved children.,"Background. The prevalence of childhood overweight in the United States has increased by 50% in the last two decades (Schumann, Nichols X Lingston, 2002) and 25% to 30% of U.S. children between the ages of 6 and 17 are obese (Moran, 1999). Several studies show that sedentary activities such as television viewing, using a computer, and playing video games increase the risk of childhood obesity (Ebbeling, Pawlak & Ludwig, 2002; Dennison, Erb & Jenkins, 2002; Yackel, 2003). An innovative means of reducing sedentary activity in children through the use of video 'exergames' or 'exertainment' is becoming a popular means of increasing physical activity in children. A limited number of studies show that 'exergames' can make a positive contribution to players'stress management, weight management, fitness, and health (Lieberman, 2006). More research is needed to determine if and how these interactive video games improve physical activity and overall health in children at high risk for obesity. Purpose. The purpose of this pilot study was to assess the effects of a four-week interactive video 'exertainment' program in underserved children, as part of an after-school program based on social cognitive theory. Method. Participants consisted of 58 children recruited from an existing after-school program in the San Bernardino City Unified School District. The participants were children ages 7-12 years of lower socioeconomic status living in Southern California. Pre- and post-assessments were made to see if there were improvements in attitude, self-efficacy, outcome expectations, body composition, cardiovascular endurance, body image, physical activity outside the intervention, absenteeism, and academic achievement. Analyses. Results from this study showed that there was a 16% decrease in resting heart rate from baseline to post-intervention for males (p= < =.001) and a 12.9% decrease in resting heart rate for females (p= < =.001). Average active heart rate decreased for all subjects post-intervention representing at 7.16% decrease for males (p=041) and 5.2% decrease for females ( p=053). Absenteeism rates improved significantly for students participating in the intervention. The mean number of days students were absent during the intervention (3.73, p=020) decreased compared to prior to starting the intervention (000, p=000). Academic scores showed significant improvements in the students' mean math performance while participating in the intervention (t=3.63, p=001) compared to non-participants (t=2.16, p=.042). The average percent body fat in males showed a non-significant decrease by less than 1% (-.89, p=.295). There were no significant changes on the other study variables. Application to Health Education. The 'U Got 2 Move It' program focused on promoting healthy lifestyle modifications in America's youth as a means of reducing the risk of obesity in underserved children by introducing an appealing format for activity--interactive video games in after-school programs. The results from this pilot test will serve as a model for health educators to partner with schools and community-based organizations to develop future health-related programs focusing on preventing or reducing the threat of obesity and related health problems in high risk children.",2007,"Young, Tammy L",THES,NA
proquest_dt,33388310,Physiological responding in a two-dimensional social interaction simulation.,"Although there were no observed effects, several suggestions were made to inform researchers in designing a study of fluid interaction. With the increasing usage of computers in the conduct of research, there is ample evidence to suggest that some emotional or physiological responses may be reliably measured using two-dimensional computer simulations. It was predicted in accordance with the previous aggression literature that when the participant was in the role of aggressor that autonomic nervous system (ANS) responses would decrease as a function of concentration or cognitive load. It was also predicted that when the participant was in the role of virtual victim, a significant increase in ANS function would be observed as a manifestation of the so termed 'flight'response were recruited. A two-dimensional virtual challenge arena was programmed using a video game interface. Fourty-two college students with no prior history of pathology alternatively attacked and defended against an attacker during which, SCL and HR were measured. General recreational data and computer usage as well as valence information was also collected. Results showed that the interaction was rated as 'somewhat pleasant'regardless of which role the participant played. They also rated the conflict as 'somewhat excited' and felt 'mostly in control' of the interaction. Overall the interaction was rated as being, 'slightly unrealistic'. Physiological measurements were made during five discrete phases. Although statistically significant differences were observed between the measurement phases verifying that the phases are likely to be meaningful, there were no reliably significant differences between the different conflict conditions in measuring heart rate and skin conductance. It was proposed that several possibilities, including too complex a measurement design, non-discrete measurement phases and individual variances in acclimation and perceived novelty of the task may have resulted in the failure to find meaningful group differences among conditions.",2007,"Brannon, Sean",THES,NA
proquest_dt,304854171,Distributed trust management in autonomic networks,"The management of autonomic networks has gained more and more attentions because of their wide applications and control difficulties. Autonomic networks are decentralized and self-organized. Without global knowledge on the states of autonomic networks, it is difficult to predict behaviors of such networks and thus to conduct proper network management and control. This dissertation is the starting point of my effort to theoretically understand the complex characteristics of autonomic networks. In particular, I focus on a specific application: distributed trust management.    We view trust among users as a set of relations established on the basis of trust credentials and required by specified policies. Two important components of a distributed trust management system are studied in this work: trust credential distribution and trust evaluation. In autonomic networks, trust credentials are distributed throughout the network. Given the mobility and dynamics of the networks, it is important to properly distribute trust credentials such that users are able to efficiently obtain required credentials and update existing credentials. I present a trust credential distribution scheme based on network coding. After obtaining credentials in need, policies are required for users to evaluate trustworthiness of targets in a distributed way. In this dissertation, I model distributed trust evaluation as an estimation problem and trust evaluation policies based on local interactions are studied. I investigate the convergence of both deterministic and stochastic voting rules and prove their effectiveness with the present of misbehaving users.    Autonomic networks rely on collaboration among users. The conflict between the benefit from collaboration and the required cost for collaboration naturally leads to game-theoretic studies. I study collaboration based on cooperative games with communication constraints and give the conditions under which users are willing to collaborate. The results in this dissertation show that a well-designed trust management system is helpful to enforce collaboration. Besides collaboration, I show that trust can be used to the utility optimization problems as well. The effect of trust values is that in the routing and scheduling problems the trustworthiness of the node will be automatically considered and used. For example, packets will not be routed frequently to suspicious nodes.",2007,"Jiang, Tao",THES,Ph.D.
proquest_dt,304839847,Human control of cooperating robots,"Advances in robotic technologies and artificial intelligence are allowing robots to emerge from research laboratories into our lives. Experiences with field applications show that we have underestimated the importance of human-robot interaction (HRI) and that new problems arise in HRI as robotic technologies expand. This thesis classifies HRI along four dimensions—human, robot, task, and world and illustrates that previous HRI classifications can be successfully interpreted as either about one of these elements or about the relationship between two or more of these elements. Current HRI studies of single-operator single-robot (SOSR) control and single-operator multiple-robots (SOMR) control are reviewed using this approach.    Human control of multiple robots has been suggested as a way to improve effectiveness in robot control. Unlike previous studies that investigated human interaction either in low-fidelity simulations or based on simple tasks, this thesis investigates human interaction with cooperating robot teams within a realistically complex environment. USARSim, a high-fidelity game-engine-based robot simulator, and MrCS, a distributed multirobot control system, were developed for this purpose. In the pilot experiment, we studied the impact of autonomy level. Mixed initiative control yielded performance superior to fully autonomous and manual control.    To avoid limitation to particular application fields, the present thesis focuses on common HRI evaluations that enable us to analyze HRI effectiveness and guide HRI design independently of the robotic system or application domain. We introduce the interaction episode (IEP), which was inspired by our pilot human-multirobot control experiment, to extend the Neglect Tolerance model to support general multiple robots control for complex tasks. Cooperation Effort (CE), Cooperation Demand (CD), and Team Attention Demand (TAD) are defined to measure the cooperation in SOMR control. Two validation experiments were conducted to validate the CD measurement under tight and weak cooperation conditions in a high-fidelity virtual environment. The results show that CD, as a generic HRI metric, is able to account for the various factors that affect HRI and can be used in HRI evaluation and analysis.",2007,"Wang, Jijun",THES,Ph.D.
proquest_dt,304762020,A Bayesian approach to multiagent reinforcement learning and coalition formation under uncertainty,"Sequential decision making under uncertainty is always a challenge for autonomous agents populating a multiagent environment, since their behaviour is inevitably influenced by the behaviour of others. Further, agents have to constantly struggle to find the right balance between exploiting  current information regarding the environment and the rest of its inhabitants, and exploring so that they acquire additional information. Moreover, they need to profitably trade off short-term rewards with anticipated long-term ones, while learning through interaction about the environment and others—employing techniques from reinforcement learning (RL), a fundamental area of study within artificial intelligence (AI).  Coalition formation is a problem of great interest within game theory and AI, allowing autonomous individually rational agents to form stable or transient teams (or coalitions) to tackle an underlying task. Agents participating in realistic scenarios of repeated  coalition formation under uncertainty face the issues identified above, and need to bargain to succesfully negotiate the terms of their participation in coalitions—often having to compromise individual with team welfare effectively.    In this thesis, we provide theoretical and algorithmic tools to accommodate  sequential decision making under uncertainty in multiagent settings, dealing with the issues above. Specifically, we combine multiagent Bayesian RL with game theoretic ideas to facilitate the agents' sequential decision making. We deal with popular multiagent problems which were to date not tackled under uncertainty, or more specifically under type uncertainty.  In our work, we assume that the environment dynamics or the types (capabilities) of other agents are not known, and thus the agents have to account for this uncertainty, in a Bayesian way, when making decisions. Handling type uncertainty allows information about others acquired within one setting to be exploited in possibly different settings in the future.    The core of our contributions lies in the area of coalition formation under uncertainty. We studied several aspects of both the cooperative and non-cooperative facets of this problem, coining new theoretical concepts, proving theoretical results, presenting and evaluating algorithms for use in this context, and proposing a Bayesian RL framework for optimal repeated coalition formation under uncertainty.",2007,"Chalkiadakis, Georgios",THES,Ph.D.
proquest_dt,304719335,ISP's traffic engineering and peering strategy,"The Internet has quickly evolved into a vast global network owned and operated by thousands of interconnected Internet Service Providers. Each of these ISPs, as one autonomous system, has its individual economic interests. ISPs can achieve their objectives through peering strategy  and interdomain traffic engineering. These two issues are important for ISPs' business and have significant implications on the Internet architecture.   Our study on interdomain traffic engineering focuses on AS Path Prepending (ASPP), a popular way for inbound traffic engineering. In order to improve the current situation that ISPs often practise this approach in a trial-and-error basis, we propose a greedy algorithm to help ISPs perform this approach systematically and efficiently. Then we demonstrate two fundamental issues of decentralized selfish traffic engineering, routing instability and  global network performance degradation, based on an abstract model where ISPs perform traffic engineering for their individual load balance. We also present a real-world pathologic case of prepending instability from our measurement study. Some simple guidelines are given for ISPs to avoid such routing instability.   Over the past several years, numerous types of “overlay” networks change the interdomain traffic pattern and ISPs lose the routing control of some interdomain traffic flows due to the application layer routing. As a result, some ISPs may provide unintended transit service for other local ISPs. It upsets the traditional business model and makes ISPs' peering strategies more complicated.   Our work on peering strategy is to help ISPs understand the economic implications of various traffic patterns and make proper decisions to optimize their business. We first conduct an economic analysis for an overlay streaming network to gain some insights on the free ride phenomenon. We further improve the analysis by taking the response of subscribers into consideration and formulate the dynamic market as a multi-leader-follower game to capture the Nash Equilibrium of the routing tussle among the major players of the Internet marketplace. Based on this framework together with a gravity traffic model, we present some important observations on the implications of overlays on ISPs' peering strategy.",2007,"Wang, Hui",THES,Ph.D.
proquest_dt,304708497,Game theoretical data replication techniques for large-scale autonomous distributed computing systems,"Data replication in geographically dispersed servers is an essential technique for reducing the user perceived access time in large-scale distributed computing systems. A majority of the conventional replica placement techniques lack scalability and solution quality. To counteract such issues, this thesis proposes a game theoretical replica placement framework, in which autonomous agents compete for the allocation or reallocation of replicas onto their representative servers in a self-managed fashion. Naturally, each agent's goal is to maximize its own benefit. However, the framework is designed to suppress individualism and to ensure system-wide optimization. Using this framework as an environment, several cooperative and non-cooperative low-complexity, flexible, and scalable game theoretical replica placement techniques are proposed, analytically investigated, and experimentally evaluated. Each of these techniques supports different game theoretical (pareto-optimality, catering to agents' interests, deliberate discrimination of allocation, budget balanced, pure Nash equilibrium, and Nash equilibrium) and system (link distance, congestion control, minimization of communication cost, and memory optimization) related properties. Using a detailed test-bed involving eighty various network topologies and two real-world access logs, each game theoretical technique is also extensively compared with conventional replica placement techniques, such as, greedy heuristics, branch-and-bound techniques and genetic algorithms. The experimental study confirms that in each case the proposed techniques outperform other conventional methods. The results can be summarized in four ways: (1) The number of replicas in a system self-adjusts to reflect the ratio of the number of reads versus writes access; (2) Performance is improved by replicating objects to the servers based on the locality of reference; (3) Replica allocations are made in a fast algorithmic turn-around time; (4) The complexity of the data replication problem is decreased by multifold.",2007,"Khan, Samee Ullah",THES,Ph.D.
proquest_dt,33277126,Modeling natural phenomena with lattice Boltzmann method.,"We have adopted a numerical method in computational fluid dynamics, the Lattice Boltzmann Method (LBM), for simulation and visualization of natural phenomena. The LBM is an alternative and promising numerical scheme for modeling the fluid dynamics from a microscopic perspective, and recovering the Navier-Stokers equations. The LBM has the following obvious advantages: (1)it uses local and simple operators to model complex and nonlinear fluid behaviors; (2)it discretizes the micro-physics of local interactions between fluids and objects, and thus can handle very complex boundary conditions, such as deep urban canyons, curved walls, arbitrarily-shaped objects and dynamic boundaries of moving objects; (3)due to its discrete nature, the LBM lends itself to multi-resolution approaches, and its computational pattern, is easily parallelizable. In addition to the traditional Single-relaxation-time LBM, we introduce a more general version: the Multiple-relaxation-time LBM, which is more appropriate for coupling physical properties (temperature, body forces, etc.) to fluid dynamics and it provides more stable computation. We have applied this method to model light objects floating in the wind, heat shimmering of air, mirage, solid melting and flowing, and contaminant plume dispersion in urban environment. We have accelerated the LBM on commodity graphics processing units (GPUs), achieving interactive performance for our applications. Moreover, we have modelled the front spreading phenomena of fire and flows on 3D surfaces. Our method provides fast and simple simulations and allows users to conveniently control the propagation behaviors. Our LBM-based approaches provide a physically based framework for modeling and simulating natural phenomena that enables the development of computer graphics, movies, games, and scientific prediction simulations.",2006,"Zhao, Ye",THES,NA
proquest_dt,33201757,A framework for element-based computer graphics.,"There is an imbalance between personal and algorithmic control in existing techniques for element-based computer graphics. A refraining of the traditional graphics pipeline is used to address this. By turning modelling and rendering into an interactive process, and introducing semi-autonomous agents to manage elements, we develop a framework through which it is possible to build applications with a given control balance. Agents negotiate control of their representations on a canvas via interactions in agent space. Interaction with the rendered image on canvas is conveyed to agents responsible for that image, which interpret the interaction and adjust the underlying model appropriately. In addition, the concept of a coalition, as in game theory, provides a means for groups of agents to organise or be organised. This framework is realised as a prototype engine along with a series of examples that illustrate a broad range of variations in control balance that are possible within this framework.",2006,"Mason, Katherine Merle",THES,NA
proquest_dt,304980155,Evolving visibly intelligent behavior for embedded game agents,"Machine learning has proven useful for producing solutions to various problems, including the creation of controllers for autonomous intelligent agents. However, the control requirements for an intelligent agent sometimes go beyond the simple ability to complete a task, or even to complete it efficiently: An agent must sometimes complete a task in style. For example, if an autonomous intelligent agent is embedded in a game where it is visible to human observers, and plays a role that evokes human intuitions about how that role should be fulfilled, then the agent must fulfill that role in a manner that does not dispel the illusion of intelligence for the observers. Such visibly intelligent behavior is a subset of general intelligent behavior: a subset that we must be able to provide if our methods are to be adopted by the developers of games and simulators.    This dissertation continues the tradition of using neuroevolution to train artificial neural networks as controllers for agents embedded in strategy games or simulators, expanding that work to address selected issues of visibly intelligent behavior. A test environment is created and used to demonstrate that modified methods can create desirable behavioral traits such as flexibility, consistency, and adherence to a doctrine, and suppress undesirable traits such as seemingly erratic behavior and excessive predictability. These methods are designed to expand a program of work leading toward adoption of neuroevolution by the commercial gaming industry, increasing player satisfaction with their products, and perhaps helping to set AI forward as The Next Big Thing in that industry. As the capabilities of research-grade machine learning converge with the needs of the commercial gaming industry, work of this sort can be expected to expand into a broad and productive area of research into the nature of intelligence and the behavior of autonomous agents.",2006,"Bryant, Bobby Don",THES,Ph.D.
proquest_dt,304956392,Impact of genetic predisposition and environmental stress on measures of preclinical essential hypertension,"The main objective of this project was to determine the impact of genetic risk and chronic environmental stress on measures of preclinical essential hypertension (EH) (e.g., exaggerated cardiovascular reactivity, increased resting hemodynamics and increased left ventricular mass [LVM]). A secondary objective was to evaluate the moderating and interactive effects of ethnicity, gender, body mass index [BMI] and anger expression on EH risk indices. Two genes with relevance for blood pressure (BP) control (e.g., beta-2 adrenergic receptor [ADRB2] gene and serotonin transporter [5-HTT] gene) were used to define genetic risk. Chronic environmental stress was assessed by socioeconomic status (SES) and subjective social status (SSS). The project consisted of three sequential studies on a large, multiethnic cohort of young adults (N>500). The first two studies were cross-sectional and based on the analysis of cardiovascular reactivity, resting hemodynamics and LVM data collected at a single visit. The third study was longitudinal and involved the tracking of BP and LVM over a 15-year span from childhood to early adulthood.    In the first study, ADRB2 haplotype significantly interacted with anger suppression in African Americans such that high anger suppressing carriers had the highest resting SBP (p<.05) and TPR reactivity to a cold pressor task (p<.01). In European Americans, ADRB2 haplotype significantly interacted with BMI to predict resting hemodynamics, such that carriers who were high in BMI showed the highest SBP (p<.05). In the second study, a significant interaction between the 5-HTT promoter region polymorphism (5-HTTLPR) and social status was found for cardiovascular reactivity, such that S allele homozygotes who were low in SES and high in SSS exhibited the greatest BP and TPR reactivity to the video game stressor (p-values<.05). No significant interaction was found between 5-HTTLPR and social status in the longitudinal study, however a significant 5-HTTLPR by BMI interaction was determined for LVM, such that obese LL homozygotes had the greatest LVM over time (p<.001). Results from this project expand what is currently known with regard to EH etiology and carry implications for the prevention of EH through the early consideration of genetic, environmental and demographic risk factors.",2006,"Poole, Joseph Christian",THES,Ph.D.
proquest_dt,304927658,Modeling natural phenomena with lattice Boltzmann method,"We have adopted a numerical method in computational fluid dynamics, the Lattice Boltzmann Method (LBM), for simulation and visualization of natural phenomena. The LBM is an alternative and promising numerical scheme for modeling the fluid dynamics from a microscopic perspective, and recovering the Navier-Stokers equations. The LBM has the following obvious advantages: (1) it uses local and simple operators to model complex and nonlinear fluid behaviors; (2) it discretizes the micro-physics of local interactions between fluids and objects, and thus can handle very complex boundary conditions, such as deep urban canyons, curved walls, arbitrarily-shaped objects and dynamic boundaries of moving objects; (3) due to its discrete nature, the LBM lends itself to multi-resolution approaches, and its computational pattern, is easily parallelizable. In addition to the traditional Single-relaxation-time LBM, we introduce a more general version: the Multiple-relaxation-time LBM, which is more appropriate for coupling physical properties (temperature, body forces, etc.) to fluid dynamics and it provides more stable computation. We have applied this method to model light objects floating in the wind, heat shimmering of air, mirage, solid melting and flowing, and contaminant plume dispersion in urban environment. We have accelerated the LBM on commodity graphics processing units (GPUs), achieving interactive performance for our applications. Moreover, we have modelled the front spreading phenomena of fire and flows on 3D surfaces. Our method provides fast and simple simulations and allows users to conveniently control the propagation behaviors. Our LBM-based approaches provide a physically based framework for modeling and simulating natural phenomena that enables the development of computer graphics, movies, games, and scientific prediction simulations.",2006,"Zhao, Ye",THES,Ph.D.
